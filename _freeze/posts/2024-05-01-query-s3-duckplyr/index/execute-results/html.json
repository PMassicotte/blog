{
  "hash": "5a48dd7a94f7e6baee8bb84f1e838efd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'When duckdb meets dplyr!'\nauthor: 'Philippe Massicotte'\ndate: '2024-05-01'\ncategories: [R, DuckDB, dplyr, duckplyr, S3, Parquet]\n# The preview file needs to be named preview.png\n# mogrify -format png preview.jpg\n# https://quarto.org/docs/websites/website-tools.html#preview-images\nimage: 'img/preview.png'\neditor_options:\n  chunk_output_type: console\ncitation: true\n---\n\n\n\n\n## Introduction\n\nI like [DuckDB](https://duckdb.org/) &#x1F986;. I am excited to see that it is [now possible](https://duckdb.org/2024/04/02/duckplyr.html) to use it with `dplyr` using the fantastic `duckplyr` [package](https://duckdblabs.github.io/duckplyr/) which gives us another way to bridge `dplyr` with DuckDB.\n\n[![When duckdb meets dplyr! Photo by DuckDB](https://duckdb.org/images/blog/duckplyr/duckplyr.png)](https://duckdb.org/2024/04/02/duckplyr.html){fig-alt=\"Hex logos of dplyr and duckdb\" fig-align=\"center\"}\n\nIn this short post, I will show how `duckplyr` can be used to query parquet files hosted on an S3 bucket. I will use the `duckplyr_df_from_parquet()` function to read the data and then use `dplyr` verbs to summarize the data.\n\nFirst, let's install the `duckplyr` package and load the necessary libraries.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"duckplyr\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(duckplyr)\nlibrary(duckdb)\n\noptions(duckdb.materialize_message = FALSE)\n```\n:::\n\n\nIn order we need to follow these steps:\n\n1. Create a connection to a DuckDB database.\n2. Load the `httpfs` extension.\n3. Set the S3 region and endpoint to access the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncon <- duckplyr:::get_default_duckdb_connection()\n\nDBI::dbSendQuery(con, \"INSTALL httpfs; LOAD httpfs;\")\nDBI::dbSendQuery(\n  con,\n  \"SET s3_region='auto';SET s3_endpoint='s3.valeria.science';\"\n)\n```\n:::\n\n\n::: {.callout-note}\nNote the use of the triple colon (`:::`) to access the internal function.\n:::\n\nNow, we can read the data from the parquet file stored on the S3 bucket with the `duckplyr_df_from_parquet()` function. We can also specify the class of the output data frame with `class = class(tibble())`. In this case, I will use `tibble`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights <- duckplyr_df_from_parquet(\n  \"s3://public/flights.parquet\",\n  class = class(tibble())\n)\n\nflights\n#> # A tibble: 336,776 × 19\n#>     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>    <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#>  1  2013     1     1      517            515         2      830            819\n#>  2  2013     1     1      533            529         4      850            830\n#>  3  2013     1     1      542            540         2      923            850\n#>  4  2013     1     1      544            545        -1     1004           1022\n#>  5  2013     1     1      554            600        -6      812            837\n#>  6  2013     1     1      554            558        -4      740            728\n#>  7  2013     1     1      555            600        -5      913            854\n#>  8  2013     1     1      557            600        -3      709            723\n#>  9  2013     1     1      557            600        -3      838            846\n#> 10  2013     1     1      558            600        -2      753            745\n#> # ℹ 336,766 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n#> #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#> #   hour <dbl>, minute <dbl>, time_hour <dttm>\n```\n:::\n\n\nFrom what I understand, all the data is pulled into the memory. This could be a problem if the data is too large. What we can do is to summarize the data to know how many rows are in the table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduckplyr_df_from_parquet(\n  \"s3://public/flights.parquet\",\n  class = class(tibble())\n) |>\n  nrow()\n#> [1] 336776\n```\n:::\n\n\nOr even have a `glimpse()` of the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduckplyr_df_from_parquet(\n  \"s3://public/flights.parquet\",\n  class = class(tibble())\n) |>\n  glimpse()\n#> Rows: 336,776\n#> Columns: 19\n#> $ year           <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n#> $ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ dep_time       <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n#> $ sched_dep_time <int> 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n#> $ dep_delay      <dbl> 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n#> $ arr_time       <int> 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n#> $ sched_arr_time <int> 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n#> $ arr_delay      <dbl> 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n#> $ carrier        <chr> \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n#> $ flight         <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n#> $ tailnum        <chr> \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n#> $ origin         <chr> \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n#> $ dest           <chr> \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n#> $ air_time       <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n#> $ distance       <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n#> $ hour           <dbl> 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n#> $ minute         <dbl> 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n#> $ time_hour      <dttm> 2013-01-01 10:00:00, 2013-01-01 10:00:00, 2013-01-01 1…\n```\n:::\n\n\nNow, let's summarize the data by calculating the average departure delay by `carrier`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights |>\n  summarise(mean_dep_delay = mean(dep_delay), .by = \"carrier\")\n#> # A tibble: 16 × 2\n#>    carrier mean_dep_delay\n#>    <chr>            <dbl>\n#>  1 9E               16.7 \n#>  2 F9               20.2 \n#>  3 YV               19.0 \n#>  4 AA                8.59\n#>  5 DL                9.26\n#>  6 B6               13.0 \n#>  7 EV               20.0 \n#>  8 WN               17.7 \n#>  9 FL               18.7 \n#> 10 AS                5.80\n#> 11 UA               12.1 \n#> 12 MQ               10.6 \n#> 13 VX               12.9 \n#> 14 HA                4.90\n#> 15 US                3.78\n#> 16 OO               12.6\n\nduckplyr_df_from_parquet(\n  \"s3://public/flights.parquet\",\n  class = class(tibble())\n) |>\n  summarise(mean_dep_delay = mean(dep_delay), .by = \"carrier\")\n#> # A tibble: 16 × 2\n#>    carrier mean_dep_delay\n#>    <chr>            <dbl>\n#>  1 9E               16.7 \n#>  2 F9               20.2 \n#>  3 YV               19.0 \n#>  4 AA                8.59\n#>  5 DL                9.26\n#>  6 B6               13.0 \n#>  7 EV               20.0 \n#>  8 WN               17.7 \n#>  9 FL               18.7 \n#> 10 AS                5.80\n#> 11 UA               12.1 \n#> 12 MQ               10.6 \n#> 13 VX               12.9 \n#> 14 HA                4.90\n#> 15 US                3.78\n#> 16 OO               12.6\n```\n:::\n\n\n## Scaling Up: Analyzing a larger dataset\n\nLet's try with a much larger dataset. I will use the NYC taxi data from 2019. The data is partitioned by month and stored in parquet partitioning. But before we can process, we need to change the endpoint.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDBI::dbSendQuery(\n  con,\n  \"SET s3_region='auto';SET s3_endpoint='';\"\n)\n\nduckplyr_df_from_file(\n  \"s3://duckplyr-demo-taxi-data/taxi-data-2019-partitioned/*/*.parquet\",\n  \"read_parquet\",\n  options = list(hive_partitioning = TRUE),\n  class = class(tibble())\n) |>\n  count()\n#> # A tibble: 1 × 1\n#>          n\n#>      <int>\n#> 1 84393604\n```\n:::\n\n\nImpressive, isn't it? With DuckDB, analyzing over 80 million rows of data is a so fast &#128512;.\n\n::: {.callout-warning}\nUnless you have a lot of memory, do not run the code below (i.e. without the `count()` function). It will load the entire dataset in memory!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduckplyr_df_from_file(\n  \"s3://duckplyr-demo-taxi-data/taxi-data-2019-partitioned/*/*.parquet\",\n  \"read_parquet\",\n  options = list(hive_partitioning = TRUE),\n  class = class(tibble())\n)\n```\n:::\n\n\n:::\n\n## Performance Benchmarking\n\nHow long it took to count the rows? Let's find out &#8986;.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time({\n  duckplyr_df_from_file(\n    \"s3://duckplyr-demo-taxi-data/taxi-data-2019-partitioned/*/*.parquet\",\n    \"read_parquet\",\n    options = list(hive_partitioning = TRUE),\n    class = class(tibble())\n  ) |>\n    count()\n})\n#>    user  system elapsed \n#>   0.016   0.004   2.304\n```\n:::\n\n\nIt took **less than 3 seconds** to count the rows. This is impressive!\n\nTo wrap up, let's do an actual analysis. We will calculate the median tip percentage by the number of passengers in the taxi.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduckplyr_df_from_file(\n  \"s3://duckplyr-demo-taxi-data/taxi-data-2019-partitioned/*/*.parquet\",\n  \"read_parquet\",\n  options = list(hive_partitioning = TRUE),\n  class = class(tibble())\n) |>\n  filter(total_amount > 0) |>\n  filter(!is.na(passenger_count)) |>\n  mutate(tip_pct = 100 * tip_amount / total_amount) |>\n  summarise(\n    avg_tip_pct = median(tip_pct),\n    n = n(),\n    .by = passenger_count\n  ) |>\n  arrange(desc(passenger_count))\n#> # A tibble: 10 × 3\n#>    passenger_count avg_tip_pct        n\n#>              <dbl>       <dbl>    <int>\n#>  1               9        12.8      221\n#>  2               8        12.4      274\n#>  3               7        10.2      412\n#>  4               6        16.6  2035161\n#>  5               5        16.7  3391426\n#>  6               4        12.8  1706047\n#>  7               3        13.7  3575542\n#>  8               2        14.5 12755829\n#>  9               1        15.3 58967003\n#> 10               0        13.7  1525284\n```\n:::\n\n\n## Conclusion\n\nBy integrating DuckDB with `dplyr` via `duckplyr`, we unlock a powerful toolset for data analysis. Whether it's exploring small datasets or crunching numbers in massive datasets, DuckDB's efficiency and dplyr's versatility make for a winning combination.\n\n## Bonus: comparing the performance with dbplyr\n\nI recently stumbled across [this discussion](https://github.com/duckdblabs/duckplyr/issues/145) addressing the difference between `duckplyr` and `dbplyr`. Intrigued, I decided to compare the performance of `duckplyr` against `dbplyr`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(bench)\n\nf_dbplyr <- function() {\n  con <- dbConnect(duckdb())\n\n  dbSendQuery(con, \"INSTALL httpfs; LOAD httpfs;\")\n  dbSendQuery(con, \"SET s3_region='auto';SET s3_endpoint='';\")\n\n  df <- tbl(\n    con,\n    \"read_parquet('s3://duckplyr-demo-taxi-data/taxi-data-2019-partitioned/*/*.parquet')\"\n  )\n\n  df |>\n    filter(total_amount > 0) |>\n    filter(!is.na(passenger_count)) |>\n    mutate(tip_pct = 100 * tip_amount / total_amount) |>\n    summarise(\n      avg_tip_pct = median(tip_pct),\n      n = n(),\n      .by = passenger_count\n    ) |>\n    arrange(desc(passenger_count))\n\n  dbDisconnect(con)\n}\n\nf_duckplyr <- function() {\n  con <- duckplyr:::get_default_duckdb_connection()\n\n  dbSendQuery(con, \"INSTALL httpfs; LOAD httpfs;\")\n  dbSendQuery(\n    con,\n    \"SET s3_region='auto';SET s3_endpoint='';\"\n  )\n\n  duckplyr_df_from_file(\n    \"s3://duckplyr-demo-taxi-data/taxi-data-2019-partitioned/*/*.parquet\",\n    \"read_parquet\",\n    options = list(hive_partitioning = TRUE),\n    class = class(tibble())\n  ) |>\n    filter(total_amount > 0) |>\n    filter(!is.na(passenger_count)) |>\n    mutate(tip_pct = 100 * tip_amount / total_amount) |>\n    summarise(\n      avg_tip_pct = median(tip_pct),\n      n = n(),\n      .by = passenger_count\n    ) |>\n    arrange(desc(passenger_count))\n}\n\nmark(f_dbplyr(), f_duckplyr(), check = FALSE)\n#> # A tibble: 2 × 6\n#>   expression        min   median `itr/sec` mem_alloc `gc/sec`\n#>   <bch:expr>   <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n#> 1 f_dbplyr()      1.25s    1.25s     0.803    5.31MB    0.803\n#> 2 f_duckplyr()    6.93s    6.93s     0.144   51.34KB    0\n```\n:::\n\n\nThe initial results show that using `dbplyr` is faster than `duckplyr`. This is interesting and I will need to investigate further to understand why.\n\n<details>\n  \n<summary>Session info</summary>\n\n\n::: {.cell}\n\n```\n#> ─ Session info ───────────────────────────────────────────────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.4.0 (2024-04-24)\n#>  os       Linux Mint 21.3\n#>  system   x86_64, linux-gnu\n#>  ui       X11\n#>  language en_CA:en\n#>  collate  en_CA.UTF-8\n#>  ctype    en_CA.UTF-8\n#>  tz       America/Montreal\n#>  date     2024-05-07\n#>  pandoc   2.9.2.1 @ /usr/bin/ (via rmarkdown)\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n#>  ! package     * version date (UTC) lib source\n#>  P bench       * 1.1.3   2023-05-04 [?] RSPM\n#>  P blob          1.2.4   2023-03-17 [?] RSPM\n#>  P cachem        1.0.8   2023-05-01 [?] RSPM\n#>  P cli           3.6.2   2023-12-11 [?] RSPM\n#>  P collections   0.3.7   2023-01-05 [?] RSPM\n#>  P DBI         * 1.2.2   2024-02-16 [?] RSPM\n#>  P dbplyr      * 2.5.0   2024-03-19 [?] RSPM\n#>  P devtools      2.4.5   2022-10-11 [?] RSPM (R 4.4.0)\n#>  P digest        0.6.35  2024-03-11 [?] RSPM\n#>  P dplyr       * 1.1.4   2023-11-17 [?] RSPM\n#>  P duckdb      * 0.10.1  2024-04-02 [?] RSPM\n#>  P duckplyr    * 0.3.2   2024-03-17 [?] RSPM\n#>  P ellipsis      0.3.2   2021-04-29 [?] RSPM\n#>  P evaluate      0.23    2023-11-01 [?] RSPM\n#>  P fansi         1.0.6   2023-12-08 [?] RSPM\n#>  P fastmap       1.1.1   2023-02-24 [?] RSPM\n#>  P fs            1.6.4   2024-04-25 [?] CRAN (R 4.4.0)\n#>  P generics      0.1.3   2022-07-05 [?] RSPM\n#>  P glue          1.7.0   2024-01-09 [?] RSPM\n#>  P htmltools     0.5.8.1 2024-04-04 [?] RSPM\n#>  P htmlwidgets   1.6.4   2023-12-06 [?] RSPM\n#>  P httpuv        1.6.15  2024-03-26 [?] RSPM\n#>  P jsonlite      1.8.8   2023-12-04 [?] RSPM\n#>  P knitr         1.46    2024-04-06 [?] RSPM\n#>  P later         1.3.2   2023-12-06 [?] RSPM\n#>  P lifecycle     1.0.4   2023-11-07 [?] RSPM\n#>  P magrittr      2.0.3   2022-03-30 [?] RSPM\n#>  P memoise       2.0.1   2021-11-26 [?] RSPM\n#>  P mime          0.12    2021-09-28 [?] RSPM\n#>  P miniUI        0.1.1.1 2018-05-18 [?] RSPM (R 4.4.0)\n#>  R nvimcom     * 0.9.41  <NA>       [?] <NA>\n#>  P pillar        1.9.0   2023-03-22 [?] RSPM\n#>  P pkgbuild      1.4.4   2024-03-17 [?] RSPM (R 4.4.0)\n#>  P pkgconfig     2.0.3   2019-09-22 [?] RSPM\n#>  P pkgload       1.3.4   2024-01-16 [?] RSPM (R 4.4.0)\n#>  P processx      3.8.4   2024-03-16 [?] RSPM\n#>  P profmem       0.6.0   2020-12-13 [?] RSPM\n#>  P profvis       0.3.8   2023-05-02 [?] RSPM (R 4.4.0)\n#>  P promises      1.3.0   2024-04-05 [?] RSPM\n#>  P ps            1.7.6   2024-01-18 [?] RSPM\n#>  P purrr         1.0.2   2023-08-10 [?] RSPM\n#>  P quarto      * 1.4     2024-03-06 [?] RSPM\n#>  P R.cache       0.16.0  2022-07-21 [?] RSPM\n#>  P R.methodsS3   1.8.2   2022-06-13 [?] RSPM\n#>  P R.oo          1.26.0  2024-01-24 [?] RSPM\n#>  P R.utils       2.12.3  2023-11-18 [?] RSPM\n#>  P R6            2.5.1   2021-08-19 [?] RSPM\n#>  P Rcpp          1.0.12  2024-01-09 [?] RSPM\n#>  P remotes       2.5.0   2024-03-17 [?] RSPM (R 4.4.0)\n#>  P renv          1.0.7   2024-04-11 [?] RSPM (R 4.4.0)\n#>  P rlang         1.1.3   2024-01-10 [?] RSPM\n#>  P rmarkdown     2.26    2024-03-05 [?] RSPM\n#>  P rstudioapi    0.16.0  2024-03-24 [?] RSPM\n#>  P sessioninfo   1.2.2   2021-12-06 [?] RSPM (R 4.4.0)\n#>  P shiny         1.8.1.1 2024-04-02 [?] RSPM (R 4.4.0)\n#>  P stringi       1.8.3   2023-12-11 [?] RSPM\n#>  P stringr       1.5.1   2023-11-14 [?] RSPM\n#>  P styler      * 1.10.3  2024-04-07 [?] RSPM\n#>  P tibble        3.2.1   2023-03-20 [?] RSPM\n#>  P tidyselect    1.2.1   2024-03-11 [?] RSPM\n#>  P urlchecker    1.0.1   2021-11-30 [?] RSPM (R 4.4.0)\n#>  P usethis       2.2.3   2024-02-19 [?] RSPM (R 4.4.0)\n#>  P utf8          1.2.4   2023-10-22 [?] RSPM\n#>  P vctrs         0.6.5   2023-12-01 [?] RSPM\n#>  P withr         3.0.0   2024-01-16 [?] RSPM\n#>  P xfun          0.43    2024-03-25 [?] RSPM\n#>  P xtable        1.8-4   2019-04-21 [?] RSPM (R 4.4.0)\n#>  P yaml          2.3.8   2023-12-11 [?] RSPM\n#> \n#>  [1] /tmp/RtmpOkV4IM/renv-use-libpath-33b3525a9df88\n#>  [2] /home/filoche/.cache/R/renv/sandbox/linux-linuxmint-jammy/R-4.4/x86_64-pc-linux-gnu/9a444a72\n#> \n#>  P ── Loaded and on-disk path mismatch.\n#>  R ── Package was removed from disk.\n#> \n#> ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n```\n:::\n\n\n</details>\n\n<details>\n\n<summary>renv.lock file</summary>\n\n```{.json include=\"renv.lock\"}\n\n```\n\n</details>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}