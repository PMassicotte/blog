---
title: 'Unlocking Cloud Data: A Step-by-Step Guide to Reading and Transforming Spatial Data with DuckDB'
author: 'Philippe Massicotte'
date: '2023-09-14'
categories: [R, Geospatial, DuckDB, Spatial, Simple Features, Parquet]

# The preview file needs to be named preview.png
# mogrify -format png preview.jpg
# https://quarto.org/docs/websites/website-tools.html#preview-images
image: 'https://repository-images.githubusercontent.com/138754790/fdc92700-357b-11eb-9761-54b3c051137c'
editor_options:
  chunk_output_type: console
citation: true
---

```{r}
#| label: renv
#| include: false
# https://www.joelnitta.com/posts/2024-01-11_using_renv_with_blog/

renv::use(lockfile = "renv.lock")

library(quarto)
library(styler)
```

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(ggpmthemes)

theme_set(theme_minimal(base_family = "Montserrat"))
```

```{r}
#| eval: false
#| include: false
#| echo: false

dbExecute(conn, "INSTALL httpfs; LOAD httpfs; SET s3_region = 'us-east-1'")

query <- paste(
  "CREATE OR REPLACE TABLE airports as SELECT * FROM",
  "read_parquet('s3://duckdb-md-dataset-121/airports_daily_top_10.parquet')"
)
```

In this blog post, I will show you how to read and transform spatial data hosted in the cloud with [DuckDB](https://duckdb.org/)^[DuckDB preview image: https://archive.org/details/github.com-duckdb-duckdb_-_2022-08-11_05-54-07]. I will use the [nycflights13](https://github.com/tidyverse/nycflights13), a popular dataset for learning [data science](https://r4ds.hadley.nz/). I will also use the [sf](https://github.com/r-spatial/sf/) package to manipulate spatial data. We will first start our journey by using DuckDB to read tabular data from parquet files from an online repository. Then with [dbplyr](https://github.com/tidyverse/dbplyr/) as a backbend, we will query the data. Then, with the help of the [DuckDB spatial extension](https://duckdb.org/docs/extensions/spatial), I will introduce how we can do geospatial processing within DuckDB and sf.

## What is DuckDB?

DuckDB is an embeddable SQL OLAP database management system designed for efficient data querying and manipulation. It excels in processing large datasets swiftly and is particularly adept at handling complex analytical tasks while offering the flexibility to seamlessly integrate with various data [sources and formats](https://duckdb.org/docs/archive/0.8.1/data/overview). DuckDB also comes with a set of [extensions](https://duckdb.org/docs/extensions/overview):

```{r}
#| echo: false
conn <- DBI::dbConnect(duckdb::duckdb())

DBI::dbSendQuery(conn, "SELECT * FROM duckdb_extensions();") |>
  DBI::dbFetch() |>
  as_tibble() |>
  select(-install_path) |>
  gt::gt()

DBI::dbDisconnect(conn, disconnect = TRUE)
```

![These ducklings hatched the same morning as this walk. They walked from their nest to the nearby pond, across neighbourhood streets](img/img1.png){width=85%}

<center>Photo by <a href="https://unsplash.com/@tchompalov?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Vlad Tchompalov</a> on <a href="https://unsplash.com/photos/wt5Y8VY_0bA?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a></center>

## Create a DuckDB remote connection

For the following example, I will work with the nycflights13 data saved in parquet file format and hosted inside S3 buckets. I will use the [httpfs](https://duckdb.org/docs/extensions/httpfs) extension to read the data. The httpfs extension allows DuckDB to read data from remote HTTP(S) sources which is exactly what we need here to read that data from the cloud.

The first thing I need to do is to create a DuckDB connection using `DBI::dbConnect()` and install the httpfs extension and load it.

```{r}
library(duckdb)
library(DBI)

# Create a connection
conn <- DBI::dbConnect(duckdb())

# Install the httpfs extension and load it
dbExecute(conn, "INSTALL httpfs; LOAD httpfs;")
```

Now that we have a connection, we can start exploring the data! All tables are stored in [parquet files](https://en.wikipedia.org/wiki/Apache_Parquet) and hosted in s3 buckets. Here are the links to the data:

- [airlines](https://nycflights13.s3.valeria.science/airlines/20230913T174717Z-23aeb/airlines.parquet)
- [airports](https://nycflights13.s3.valeria.science/airports/20230913T174717Z-ec372/airports.parquet)^[Note that the airports table had positive longitudes. It seems that they should all be negative, so I corrected them.]
- [flights](https://nycflights13.s3.valeria.science/flights/20230913T174716Z-3899c/flights.parquet)
- [planes](https://nycflights13.s3.valeria.science/planes/20230913T174717Z-b5438/planes.parquet)
- [weather](https://nycflights13.s3.valeria.science/weather/20230913T174717Z-01531/weather.parquet)

![I'm @eodiin on Instagram](img/img2.png){width=85%}

<center>Photo by <a href="https://unsplash.com/@odiin?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Erik Odiin</a> on <a href="https://unsplash.com/photos/jbQvJx2EWnU?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a></center>

<br>

I will start by reading the `flights` table using the [SQL](https://duckdb.org/docs/sql/introduction) syntax. To do so, I will use the `read_parquet()` function from DuckDB to read the parquet file and return a DuckDB table. Then, I will use the `CREATE OR REPLACE TABLE` statement to create a new table called `flights` and store the data returned by `read_parquet()`.

```{r}
query <- paste(
  "CREATE OR REPLACE TABLE flights as SELECT * FROM",
  "read_parquet('https://nycflights13.s3.valeria.science/flights/20230913T174716Z-3899c/flights.parquet')"
)

dbExecute(conn, query)
```

We can verify that the table was created by using the `SHOW ALL TABLES` statement.

```{r}
dbSendQuery(conn, "SHOW ALL TABLES;") |>
  dbFetch() |>
  as_tibble()
```

We can further explore the table by using the following statements.

```{r}
dbSendQuery(conn, "FROM flights LIMIT 10;") |>
  dbFetch() |>
  as_tibble()

dbSendQuery(conn, "SHOW flights;") |>
  dbFetch() |>
  as_tibble()

dbSendQuery(conn, "SUMMARIZE flights;") |>
  dbFetch() |>
  as_tibble()
```

Good, we now have created a table, we can start querying it. Let's start by calculating the average delay per destination. I will show how to do it with both DuckDB and R.

## Data wrangling using DuckDB

I will use the `GROUP BY` statement to group the data by destination and then use the `AVG()` function to calculate the average delay and finally order the results with the `ORDER BY` clause.

```{r}
df1 <- dbSendQuery(
  conn,
  "SELECT dest, AVG(arr_delay) AS delay, COUNT(*) AS n
  FROM flights GROUP BY dest ORDER BY delay DESC;"
) |>
  dbFetch() |>
  as_tibble()

df1
```

## Data wrangling using dbplyr

If you prefer the syntax of [dplyr](https://dplyr.tidyverse.org/) syntax, you can use it to query DuckDB. First, we need to pull the data using the `tbl()` [function](https://dbplyr.tidyverse.org/reference/tbl.src_dbi.html).

```{r}
tbl(conn, "flights")
```

This gives us something to work with using the `dplyr` syntax. We can now use the `group_by()` function to group the data by destination and then use the `summarize()` function to calculate the average delay and the number of flights per destination. Finally, we can use the `arrange()` function to order the results by delay. But before we get the results, we can show what is the SQL query that was generated with the `show_query()` function from the [dbplyr](https://dbplyr.tidyverse.org/articles/sql.html) package.

```{r}
tbl(conn, "flights") |>
  group_by(dest) |>
  summarize(
    n = n(),
    delay = mean(arr_delay, na.rm = TRUE)
  ) |>
  arrange(desc(delay)) |>
  show_query()
```

Note that it looks pretty similar to the SQL query we wrote earlier. Now, we can get the results by using the `collect()` function.

```{r}
df2 <- tbl(conn, "flights") |>
  group_by(dest) |>
  summarize(
    delay = mean(arr_delay, na.rm = TRUE),
    n = n()
  ) |>
  arrange(desc(delay)) |>
  collect()

df2
```

We can verify that both data frames are the same.

```{r}
identical(df1, df2)
```

### Create a local database

Up to now, we have been working with an in-memory database. This means that once we close R, all the data will be lost. But what if we want to work with a local database? Well, it is pretty easy to do. We just need to specify the path to the database file when we create our connection. We can also specify if we want to create a read-only database or not.

```{r}
#| eval: false

library("DBI")

# to start an in-memory database
con <- dbConnect(duckdb::duckdb(), dbdir = ":memory:")

# to use a database file (not shared between processes)
con <- dbConnect(duckdb::duckdb(), dbdir = "my-db.duckdb", read_only = FALSE)

# to use a database file (shared between processes)
con <- dbConnect(duckdb::duckdb(), dbdir = "my-db.duckdb", read_only = TRUE)
```

## Let's go spatial ðŸš€ðŸš€ðŸš€

If you know the nycflights13 dataset, you have noticed that the `airports` table has a `lon` and `lat` columns. This means that we can use these columns to create a geometry column and do some geospatial processing. To do so, we will need to install the [spatial extension](https://duckdb.org/docs/extensions/spatial) and load it. But first, let's create the `airports` table as we did for the `flights` table.

```{r}
query <- paste(
  "CREATE OR REPLACE TABLE airports as SELECT * FROM",
  "read_parquet('https://nycflights13.s3.valeria.science/airports/20230913T174717Z-ec372/airports.parquet')"
)

dbExecute(conn, query)
```

We can verify that we now have two tables.

```{r}
dbSendQuery(conn, "SHOW ALL TABLES;") |>
  dbFetch() |>
  as_tibble()
```

And also have a glimpse of the data.

```{r}
dbSendQuery(conn, "SHOW airports;") |>
  dbFetch() |>
  as_tibble()

dbSendQuery(conn, "SHOW airports;") |>
  dbFetch()
```

At this point, this is a simple table with no geometry column. To create the geometry column, we first need to install the spatial extension and load it.

```{r}
dbExecute(conn, "INSTALL 'spatial'; LOAD 'spatial';")
```

This is now the time to let the magic happen. I will create a new table `airports_sf` and use the `ST_asWKB()` function to create the `geometry` column. The `ST_asWKB()` function takes two arguments: the geometry type and the geometry column. In our case, we will use the `ST_Point()` function to create a point geometry from the `lon` and `lat` columns that are present in the `airports` table.

```{r}
#' Here I am creating the geometry column, looks like it worked.
dbExecute(
  conn,
  "CREATE OR REPLACE TABLE airports_sf AS SELECT *,
  ST_asWKB(ST_Point(lon, lat)) as geometry, FROM airports;"
)
```

Looks like it worked! There is a `geometry` column in the `airports_sf` table. We can verify that by using the `SHOW airports_sf;` statement.

```{r}
dbSendQuery(conn, "SHOW airports_sf;") |>
  dbFetch()
```

### Querying spatial data

With the geometry created, we can now do some geospatial processing. Let's start by calculating the distance between the airports and a point located at longitude -100 and latitude 40. To do so, we will use the `ST_GeomFromWKB()` function to create a geometry from the geometry column. Finally, we will use the `ST_Distance()` function to calculate the distance between the airports and a point located at longitude -100 and latitude 40. We will order the results by decreasing distances. If we look at the [extension](https://duckdb.org/docs/extensions/spatial) documentation, we note that the `ST_Distance` function operates on two geometry. However, the geometry column is of type WKB (well-known binary) and need to be converted to `geom` with `ST_GeomFromWKB()`. Finally, we can use the `ST_Distance()` function to calculate the distance between the airports and a point located at longitude -100 and latitude 40. ^[Note that we are using lat/lon data for the calculation whereas a projected coordinate reference system would be better suited.]

```{r}
dbSendQuery(conn, "SELECT faa, name,
  st_distance(ST_Transform(ST_GeomFromWKB(geometry), 'EPSG:4326', 'EPSG:4326'),
  ST_Transform(ST_Point(-100, 40), 'EPSG:4326', 'EPSG:4326')) as dist from
  airports_sf ORDER BY dist DESC") |>
  dbFetch() |>
  as_tibble()
```

What is nice, is that it is possible to use `st_read()` from `sf` to read the data directly from DuckDB. To do so, we need to specify the connection the query (which columns from the table we want) and the geometry column.

```{r}
library(sf)

airports_sf <- st_read(
  conn,
  query = "SELECT * FROM airports_sf;",
  geometry_column = "geometry"
)

airports_sf
```

Note that if you want to write the data to a local file without using, so we can reuse it later, you can use the `COPY` statement. Here I am writing the `airports_sf` table to a GeoJSON file. Note that I am using the `LAYER_CREATION_OPTIONS` to specify that I want to write the bounding box to the file.

```{r}
#| eval: false
dbExecute(conn, "COPY  airports_sf TO '~/Desktop/airports_sf.geojson' WITH (FORMAT GDAL, DRIVER 'GeoJSON', LAYER_CREATION_OPTIONS 'WRITE_BBOX=YES')")
```

While I am there, why not visualize the data?

```{r}
library(rnaturalearth)

dest_proj <- "ESRI:102008"

usa <- ne_countries(
  country = "United States of America",
  returnclass = "sf"
) |>
  st_transform(dest_proj)


airports_sf |>
  st_set_crs(4326) |>
  ggplot() +
  geom_sf(data = usa) +
  geom_sf(size = 0.25) +
  geom_point(aes(x = -100, y = 40), size = 5, color = "red") +
  coord_sf(crs = dest_proj) +
  theme(
    axis.title = element_blank()
  )
```

Finally, do not forget to close your connection!

```{r}
dbDisconnect(conn, disconnect = TRUE)
```

## Ressources

- <https://duckdb.org/docs/extensions/spatial>
- <https://duckdb.org/2023/04/28/spatial.html>
- <https://tech.marksblogg.com/duckdb-gis-spatial-extension.html>
- <https://youtu.be/ZX5FdqzGT1E>
- <https://youtu.be/ljzpm3Mrw-I?list=PLIYcNkSjh-0ztvwoAp3GeW8HNSUSk_q3K>

<details>
  
<summary>Session info</summary>

```{r sessioninfo, echo = FALSE}
## Reproducibility info
options(width = 120)
devtools::session_info()
```

</details>
